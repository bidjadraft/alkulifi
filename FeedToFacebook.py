import requests
from bs4 import BeautifulSoup
import os
import re
import logging
import time

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ ÙˆÙ‚Ù†Ø§Ø© ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FACEBOOK_TOKEN = os.getenv("FACEBOOK_TOKEN")
FACEBOOK_PAGE_ID = os.getenv("FACEBOOK_PAGE_ID")
TELEGRAM_URL = "https://t.me/s/alkulife"
LAST_POST_FILE = "lastpost.txt"

def clean_content_text_only(soup_element):
    """ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‚ÙŠ Ù…Ù† Ø¹Ù†ØµØ± BeautifulSoup."""
    return soup_element.get_text(separator="\n", strip=True)

def is_meaningful_text(text):
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª."""
    words = text.strip().split()
    return len(words) >= 40

def read_last_post_link(filepath):
    """ÙŠÙ‚Ø±Ø£ Ø¢Ø®Ø± Ø±Ø§Ø¨Ø· Ù…Ù†Ø´ÙˆØ± ØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù…Ù† Ù…Ù„Ù."""
    if not os.path.exists(filepath):
        return None
    with open(filepath, "r", encoding="utf-8") as f:
        return f.read().strip()

def save_last_post_link(last_link, filepath=LAST_POST_FILE):
    """ÙŠØ­ÙØ¸ Ø±Ø§Ø¨Ø· Ø¢Ø®Ø± Ù…Ù†Ø´ÙˆØ± ØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ù…Ù„Ù."""
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(last_link)

def rephrase_text_with_gemini(text):
    """ÙŠØ¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API."""
    if not GEMINI_API_KEY:
        logging.error("GEMINI_API_KEY ØºÙŠØ± Ù…ØªØ§Ø­. Ø³ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ.")
        return text

    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}"
    prompt = f"Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¥Ø®Ø¨Ø§Ø±ÙŠ ÙˆÙ…Ù†Ø¸Ù… Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù†Øµ Ù…Ù‚Ø³Ù…Ù‹Ø§ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.\n\nØ§Ù„Ù†Øµ: {text}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ]
    }

    try:
        response = requests.post(api_url, json=data, headers=headers, timeout=30)
        response.raise_for_status()
        candidates = response.json().get('candidates', [])
        if candidates:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            gemini_response_text = candidates[0]['content']['parts'][0]['text']
            return gemini_response_text
    except requests.exceptions.RequestException as e:
        logging.error(f"ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini API: {e}")
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Gemini: {e}")
    
    # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„ØŒ Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ
    return text

def post_to_facebook(token, page_id, message, image_url=None):
    """ÙŠØ±Ø³Ù„ Ù…Ù†Ø´ÙˆØ±Ù‹Ø§ Ø¥Ù„Ù‰ ØµÙØ­Ø© ÙÙŠØ³Ø¨ÙˆÙƒØŒ Ù…Ø¹ Ø£Ùˆ Ø¨Ø¯ÙˆÙ† ØµÙˆØ±Ø©."""
    try:
        if image_url:
            url = f"https://graph.facebook.com/v12.0/{page_id}/photos"
            data = {
                "url": image_url,
                "published": "true",
                "access_token": token,
                "caption": message
            }
            r = requests.post(url, data=data)
        else:
            url = f"https://graph.facebook.com/v12.0/{page_id}/feed"
            data = {
                "message": message,
                "access_token": token
            }
            r = requests.post(url, data=data)
        r.raise_for_status()
        logging.info("ØªÙ… Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ Ø¨Ù†Ø¬Ø§Ø­. ğŸ‘")
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ: {e}")
        return False
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠØ³Ø¨ÙˆÙƒ: {e}")
        return False

def fetch_and_post_latest_posts():
    """ÙŠØ¬Ù„Ø¨ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… ÙˆÙŠØ¹ÙŠØ¯ ØµÙŠØ§ØºØªÙ‡Ø§ ÙˆÙŠÙ†Ø´Ø±Ù‡Ø§ Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ."""
    try:
        response = requests.get(TELEGRAM_URL)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.RequestException as e:
        logging.error(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
        return

    last_link = read_last_post_link(LAST_POST_FILE)
    messages = soup.find_all('div', class_='tgme_widget_message_wrap')
    
    filtered_msgs = []
    for msg in messages:
        text_div = msg.find('div', class_='tgme_widget_message_text')
        if not text_div:
            continue
        
        text_content = text_div.get_text()
        if 'ğŸ‘‡' in text_content or msg.find('audio') or msg.find('video'):
            continue
        
        excluded_exts = ['mp3', 'ogg', 'wav', 'mp4', 'mov', 'avi', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'zip', 'rar']
        links = msg.find_all('a')
        if any(any(a.get('href', '').lower().endswith(ext) for ext in excluded_exts) for a in links):
            continue
        
        filtered_msgs.append(msg)

    # Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ù‹Ø§
    filtered_msgs.reverse()
    
    posts_to_process = []
    for msg in filtered_msgs:
        link_tag = msg.find('a', class_='tgme_widget_message_date')
        post_link = "https://t.me" + link_tag['href'] if link_tag and link_tag['href'].startswith('/') else link_tag['href']
        
        if last_link and post_link == last_link:
            break
        
        text_div = msg.find('div', class_='tgme_widget_message_text')
        content_text = clean_content_text_only(text_div)
        
        if not is_meaningful_text(content_text):
            continue

        photo_div = msg.find('a', class_=re.compile(r'tgme_widget_message_photo_wrap'))
        image_url = None
        if photo_div:
            style = photo_div.get('style','')
            m = re.search(r"background-image:url\('([^']+)'\)", style)
            if m:
                image_url = m.group(1)

        posts_to_process.append({
            'link': post_link,
            'content': content_text,
            'image': image_url
        })
    
    if not posts_to_process:
        logging.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù†Ø´Ø±.")
        return

    # Ø§Ù„Ù†Ø´Ø± Ø¨ØªØ±ØªÙŠØ¨ Ø²Ù…Ù†ÙŠ ØµØ­ÙŠØ­ (Ù…Ù† Ø§Ù„Ø£Ù‚Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø£Ø­Ø¯Ø«)
    posts_to_process.reverse() 
    
    for post in posts_to_process:
        # Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini
        rephrased_text = rephrase_text_with_gemini(post['content'])
        
        logging.info(f"\n--- Ù…Ù†Ø´ÙˆØ± Ø¬Ø¯ÙŠØ¯ ---")
        logging.info(f"Ø§Ù„Ø±Ø§Ø¨Ø·: {post['link']}")
        logging.info(f"Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:\n{post['content']}")
        logging.info(f"Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡:\n{rephrased_text}")
        if post['image']:
            logging.info(f"Ø§Ù„ØµÙˆØ±Ø©: {post['image']}")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø¥Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ
        success = post_to_facebook(FACEBOOK_TOKEN, FACEBOOK_PAGE_ID, rephrased_text, post['image'])
        if success:
            save_last_post_link(post['link'])
            logging.info("ØªÙ… Ø­ÙØ¸ Ø¢Ø®Ø± Ù…Ù†Ø´ÙˆØ± ØªÙ… Ù†Ø´Ø±Ù‡ Ø¨Ù†Ø¬Ø§Ø­.")
        else:
            logging.error("ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø±ØŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø¢Ø®Ø± Ù…Ù†Ø´ÙˆØ±.")
        
        time.sleep(10) # Ø§Ù†ØªØ¸Ø§Ø± 10 Ø«ÙˆØ§Ù†Ù Ù‚Ø¨Ù„ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„ØªØ§Ù„ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± API

if __name__ == "__main__":
    fetch_and_post_latest_posts()