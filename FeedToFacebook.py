import requests
from bs4 import BeautifulSoup
import os
import re
import logging
import time

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ ÙˆÙ‚Ù†Ø§Ø© ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FACEBOOK_TOKEN = os.getenv("FACEBOOK_TOKEN")
FACEBOOK_PAGE_ID = os.getenv("FACEBOOK_PAGE_ID")
TELEGRAM_URL = "https://t.me/s/alkulife"
LAST_POST_FILE = "lastpost.txt"
MAX_RETRIES = 5  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø£Ù‚ØµÙ‰ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„

def clean_content_text_only(soup_element):
    """ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‚ÙŠ Ù…Ù† Ø¹Ù†ØµØ± BeautifulSoup."""
    return soup_element.get_text(separator="\n", strip=True)

def is_meaningful_text(text):
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª."""
    words = text.strip().split()
    return len(words) >= 40

def read_last_post_link(filepath):
    """ÙŠÙ‚Ø±Ø£ Ø¢Ø®Ø± Ø±Ø§Ø¨Ø· Ù…Ù†Ø´ÙˆØ± ØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù…Ù† Ù…Ù„Ù."""
    if not os.path.exists(filepath):
        return None
    with open(filepath, "r", encoding="utf-8") as f:
        return f.read().strip()

def save_last_post_link(last_link, filepath=LAST_POST_FILE):
    """ÙŠØ­ÙØ¸ Ø±Ø§Ø¨Ø· Ø¢Ø®Ø± Ù…Ù†Ø´ÙˆØ± ØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ù…Ù„Ù."""
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(last_link)

def rephrase_text_with_gemini(text):
    """ÙŠØ¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©."""
    if not GEMINI_API_KEY:
        logging.error("GEMINI_API_KEY ØºÙŠØ± Ù…ØªØ§Ø­. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ©.")
        return None

    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}"
    prompt = f"Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¥Ø®Ø¨Ø§Ø±ÙŠ ÙˆÙ…Ù†Ø¸Ù… Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŒ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø£ØµÙ„ÙŠ. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù†Øµ Ù…Ù‚Ø³Ù…Ù‹Ø§ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©.\n\nØ§Ù„Ù†Øµ: {text}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ]
    }

    for attempt in range(MAX_RETRIES):
        try:
            response = requests.post(api_url, json=data, headers=headers, timeout=30)
            response.raise_for_status()
            candidates = response.json().get('candidates', [])
            if candidates:
                gemini_response_text = candidates[0]['content']['parts'][0]['text']
                logging.info(f"Ù†Ø¬Ø­Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ù…Ø¹ Gemini ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø±Ù‚Ù… {attempt + 1}.")
                return gemini_response_text
        except requests.exceptions.RequestException as e:
            logging.warning(f"ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini API ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Gemini ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")
        
        time.sleep(5) # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø«ÙˆØ§Ù†Ù Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©

    logging.error(f"ÙØ´Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ {MAX_RETRIES} Ù…Ø­Ø§ÙˆÙ„Ø§Øª. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†Ø´ÙˆØ±.")
    return None

def post_to_facebook(token, page_id, message, image_url=None):
    """ÙŠØ±Ø³Ù„ Ù…Ù†Ø´ÙˆØ±Ù‹Ø§ Ø¥Ù„Ù‰ ØµÙØ­Ø© ÙÙŠØ³Ø¨ÙˆÙƒ Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©."""
    for attempt in range(MAX_RETRIES):
        try:
            if image_url:
                url = f"https://graph.facebook.com/v12.0/{page_id}/photos"
                data = {
                    "url": image_url,
                    "published": "true",
                    "access_token": token,
                    "caption": message
                }
                r = requests.post(url, data=data)
            else:
                url = f"https://graph.facebook.com/v12.0/{page_id}/feed"
                data = {
                    "message": message,
                    "access_token": token
                }
                r = requests.post(url, data=data)
            r.raise_for_status()
            logging.info(f"ØªÙ… Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø±Ù‚Ù… {attempt + 1}. ğŸ‘")
            return True
        except requests.exceptions.RequestException as e:
            logging.warning(f"ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")
            if r.status_code == 400 or r.status_code == 403: # Ø£Ø®Ø·Ø§Ø¡ Ù‚Ø¯ Ù„Ø§ ØªÙÙŠØ¯ ÙÙŠÙ‡Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
                 logging.error(f"Ø®Ø·Ø£ ÙÙŠØ³Ø¨ÙˆÙƒ {r.status_code}ØŒ Ù„Ù† ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©.")
                 break
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø£ ÙÙŠØ³Ø¨ÙˆÙƒ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}: {e}")

        time.sleep(5) # Ø§Ù†ØªØ¸Ø§Ø± 5 Ø«ÙˆØ§Ù†Ù Ù‚Ø¨Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©

    logging.error(f"ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ Ø¨Ø¹Ø¯ {MAX_RETRIES} Ù…Ø­Ø§ÙˆÙ„Ø§Øª.")
    return False

def fetch_and_post_latest_posts():
    """ÙŠØ¬Ù„Ø¨ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙŠØ¹ÙŠØ¯ ØµÙŠØ§ØºØªÙ‡Ø§ØŒ ÙˆÙŠÙ†Ø´Ø±Ù‡Ø§ Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ."""
    try:
        response = requests.get(TELEGRAM_URL)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.RequestException as e:
        logging.error(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
        return

    last_link = read_last_post_link(LAST_POST_FILE)
    messages = soup.find_all('div', class_='tgme_widget_message_wrap')
    
    filtered_msgs = []
    for msg in messages:
        text_div = msg.find('div', class_='tgme_widget_message_text')
        if not text_div:
            continue
        text_content = text_div.get_text()
        if 'ğŸ‘‡' in text_content or msg.find('audio') or msg.find('video'):
            continue
        excluded_exts = ['mp3', 'ogg', 'wav', 'mp4', 'mov', 'avi', 'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'zip', 'rar']
        links = msg.find_all('a')
        if any(any(a.get('href', '').lower().endswith(ext) for ext in excluded_exts) for a in links):
            continue
        filtered_msgs.append(msg)

    filtered_msgs.reverse() # Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ù‹Ø§
    
    posts_to_process = []
    for msg in filtered_msgs:
        link_tag = msg.find('a', class_='tgme_widget_message_date')
        post_link = "https://t.me" + link_tag['href'] if link_tag and link_tag['href'].startswith('/') else link_tag['href']
        
        if last_link and post_link == last_link:
            break
        
        text_div = msg.find('div', class_='tgme_widget_message_text')
        content_text = clean_content_text_only(text_div)
        if not is_meaningful_text(content_text):
            continue

        photo_div = msg.find('a', class_=re.compile(r'tgme_widget_message_photo_wrap'))
        image_url = None
        if photo_div:
            style = photo_div.get('style','')
            m = re.search(r"background-image:url\('([^']+)'\)", style)
            if m:
                image_url = m.group(1)
        posts_to_process.append({
            'link': post_link,
            'content': content_text,
            'image': image_url
        })
    
    if not posts_to_process:
        logging.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù†Ø´Ø±.")
        return

    posts_to_process.reverse() # Ø§Ù„Ù†Ø´Ø± Ø¨ØªØ±ØªÙŠØ¨ Ø²Ù…Ù†ÙŠ ØµØ­ÙŠØ­
    
    for post in posts_to_process:
        # 1. Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
        rephrased_text = rephrase_text_with_gemini(post['content'])
        if rephrased_text is None:
            # Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ© Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§ØªØŒ Ù†ØªÙˆÙ‚Ù ÙˆÙ„Ø§ Ù†Ù†Ø´Ø±
            logging.error("ÙØ´Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ù†ØµØŒ Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.")
            return # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
        
        # 2. Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
        success = post_to_facebook(FACEBOOK_TOKEN, FACEBOOK_PAGE_ID, rephrased_text, post['image'])
        
        # 3. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙ‚Ø· Ø¥Ø°Ø§ Ù†Ø¬Ø­Øª ÙƒÙ„ØªØ§ Ø§Ù„Ø¹Ù…Ù„ÙŠØªÙŠÙ†
        if success:
            save_last_post_link(post['link'])
            logging.info("ØªÙ… Ø­ÙØ¸ Ø¢Ø®Ø± Ù…Ù†Ø´ÙˆØ± ØªÙ… Ù†Ø´Ø±Ù‡ Ø¨Ù†Ø¬Ø§Ø­.")
        else:
            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒØŒ Ù†ØªÙˆÙ‚Ù
            logging.error("ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒØŒ Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.")
            return # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
        
        time.sleep(10) # Ø§Ù†ØªØ¸Ø§Ø± 10 Ø«ÙˆØ§Ù†Ù Ù‚Ø¨Ù„ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„ØªØ§Ù„ÙŠ

if __name__ == "__main__":
    fetch_and_post_latest_posts()